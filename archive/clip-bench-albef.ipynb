{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4285043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/eccv_caption/metrics.py:21: UserWarning: failed to import `ujson`. use `json` instead.\n",
      "  warnings.warn('failed to import `ujson`. use `json` instead.')\n",
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import eccv_caption\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ea3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../dataset/coco2014\"\n",
    "ANN_PATH = f\"{DATASET_PATH}/annotations/karpathy_test.json\"\n",
    "DATA_DIR = f\"{DATASET_PATH}/eccv/data\"\n",
    "MODEL_NAME = \"ViT-L-14\"\n",
    "PRETRAINED = \"openai\"\n",
    "IMAGE_BATCH = 64\n",
    "TEXT_BATCH = 256\n",
    "RETRIEVE_K = 50\n",
    "ALBEF_IMAGE_BATCH = 32\n",
    "ALBEF_CFG_PATH = \"../configs/albef_retrieval_base.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d4b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_karpathy_test(ann_path, coco_root):\n",
    "    data = json.load(open(ann_path, \"r\", encoding=\"utf-8\"))\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    captions = []\n",
    "    caption_ids = []\n",
    "\n",
    "    for img in data[\"images\"]:\n",
    "        if img[\"split\"] != \"test\":\n",
    "            continue\n",
    "        img_path = os.path.join(coco_root, img[\"filepath\"], img[\"filename\"])\n",
    "        images.append(img_path)\n",
    "        image_ids.append(img[\"cocoid\"])\n",
    "        for sent in img[\"sentences\"]:\n",
    "            captions.append(sent[\"raw\"].strip())\n",
    "            caption_ids.append(sent[\"sentid\"])\n",
    "\n",
    "    return images, image_ids, captions, caption_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0726c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(model, preprocess, image_paths, batch_size, device):\n",
    "    feats = []\n",
    "    total = len(image_paths)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = [preprocess(Image.open(p).convert(\"RGB\"))\n",
    "                  for p in batch_paths]\n",
    "        image_input = torch.stack(images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_feats = model.encode_image(image_input)\n",
    "        feats.append(batch_feats)\n",
    "        # if (i // batch_size) % 10 == 0:\n",
    "        #     done = min(i + batch_size, total)\n",
    "        #     print(f\"Encoded images: {done}/{total}\")\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    return feats / feats.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349858e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(model, tokenizer, texts, batch_size, device):\n",
    "    feats = []\n",
    "    total = len(texts)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        tokens = tokenizer(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_feats = model.encode_text(tokens)\n",
    "        feats.append(batch_feats)\n",
    "        # if (i // batch_size) % 10 == 0:\n",
    "        #     done = min(i + batch_size, total)\n",
    "        #     print(f\"Encoded captions: {done}/{total}\")\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    return feats / feats.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f76b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_faiss(text_feats, img_feats, k):\n",
    "    txf = text_feats.detach().cpu().numpy().astype(\"float32\", copy=False)\n",
    "    imf = img_feats.detach().cpu().numpy().astype(\"float32\", copy=False)\n",
    "    txf = np.ascontiguousarray(txf)\n",
    "    imf = np.ascontiguousarray(imf)\n",
    "    k = min(k, imf.shape[0])\n",
    "    index = faiss.IndexFlatIP(imf.shape[1])\n",
    "    index.add(imf)\n",
    "    _, indices = index.search(txf, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd07510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Test images: 5000\n",
      "Test captions: 25010\n",
      "Test captions: 25010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded in 286.4s\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "images, image_ids, captions, caption_ids = load_karpathy_test(\n",
    "    ANN_PATH, DATASET_PATH)\n",
    "print(f\"Test images: {len(images)}\")\n",
    "print(f\"Test captions: {len(captions)}\")\n",
    "print(f\"Test captions: {len(caption_ids)}\")\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    MODEL_NAME, pretrained=PRETRAINED, device=device)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "t0 = time.time()\n",
    "image_feats = encode_images(model, preprocess, images, IMAGE_BATCH, device)\n",
    "text_feats = encode_texts(model, tokenizer, captions, TEXT_BATCH, device)\n",
    "print(f\"Encoded in {time.time() - t0:.1f}s\")\n",
    "\n",
    "i2t_rank = topk_faiss(image_feats, text_feats, RETRIEVE_K).tolist()\n",
    "t2i_rank = topk_faiss(text_feats, image_feats, RETRIEVE_K).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c1bfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25010, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(t2i_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac7c97",
   "metadata": {},
   "source": [
    "### ALBEF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0663fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from lavis.models import load_preprocess\n",
    "from lavis.models.albef_models.albef_retrieval import AlbefRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "366a2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def albef_rerank(albef_model, albef_preprocess, captions, image_paths, t2i_rank, batch_size, device):\n",
    "    image_feats = []\n",
    "    total = len(image_paths)\n",
    "    batch_start = time.time()\n",
    "    for i in tqdm(range(0, total, batch_size), desc=\"ALBEF image feats\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = [albef_preprocess(Image.open(p).convert(\"RGB\"))\n",
    "                  for p in batch_paths]\n",
    "        image_input = torch.stack(images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_feats = albef_model.visual_encoder.forward_features(\n",
    "                image_input)\n",
    "        image_feats.append(batch_feats.cpu())\n",
    "    image_feats = torch.cat(image_feats, dim=0)\n",
    "    batch_time = time.time() - batch_start\n",
    "\n",
    "    print(f\"ALBEF image feats: {total} in {batch_time:.3f}s\")\n",
    "\n",
    "    reranked = []\n",
    "    total_caps = len(captions)\n",
    "    batch_start = time.time()\n",
    "    for i in tqdm(range(total_caps), desc=\"ALBEF rerank\"):\n",
    "        cand = t2i_rank[i]\n",
    "        img_feat = image_feats[cand].to(device)\n",
    "        encoder_att = torch.ones(\n",
    "            img_feat.size()[:-1], dtype=torch.long).to(device)\n",
    "        text_input = albef_model.tokenizer(\n",
    "            [captions[i]] * len(cand),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=35,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = albef_model.text_encoder(\n",
    "                text_input.input_ids,\n",
    "                attention_mask=text_input.attention_mask,\n",
    "                encoder_hidden_states=img_feat,\n",
    "                encoder_attention_mask=encoder_att,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            scores = albef_model.itm_head(\n",
    "                output.last_hidden_state[:, 0, :])[:, 1]\n",
    "        order = torch.argsort(scores, descending=True)\n",
    "        reranked.append([cand[j] for j in order.tolist()])\n",
    "\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"ALBEF rerank: {total_caps} in {batch_time:.3f}s\")\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d732e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/lavis310/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 256 to 576\n",
      "reshape position embedding from 256 to 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALBEF image feats: 100%|██████████| 157/157 [02:50<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBEF image feats: 5000 in 182.658s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALBEF rerank: 100%|██████████| 25010/25010 [1:37:47<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBEF rerank: 25010 in 5867.606s\n"
     ]
    }
   ],
   "source": [
    "albef_cfg = OmegaConf.load(ALBEF_CFG_PATH)\n",
    "albef_model = AlbefRetrieval.from_config(albef_cfg.model)\n",
    "albef_model.eval()\n",
    "albef_model = albef_model.to(device)\n",
    "albef_vis, _ = load_preprocess(albef_cfg.preprocess)\n",
    "\n",
    "t2i_rank = albef_rerank(\n",
    "    albef_model, albef_vis[\"eval\"], captions, images, t2i_rank, ALBEF_IMAGE_BATCH, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5134a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 1863472.54it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 2779893.96it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 1281721.06it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 1594477.14it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 1177315.44it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 1610814.80it/s]\n",
      "100%|██████████| 1261/1261 [00:00<00:00, 68491.96it/s]\n",
      "100%|██████████| 1332/1332 [00:00<00:00, 143284.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO 5K T2I recalls:\n",
      "R@1: 0.52\n",
      "R@5: 0.76\n",
      "R@10: 0.84\n",
      "ECCV T2I metrics:\n",
      "Map@R: 0.41\n",
      "R-P: 0.50\n",
      "R@1: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i2t = {\n",
    "    image_ids[i]: [caption_ids[j] for j in i2t_rank[i]]\n",
    "    for i in range(len(image_ids))\n",
    "}\n",
    "t2i = {\n",
    "    caption_ids[i]: [image_ids[j] for j in t2i_rank[i]]\n",
    "    for i in range(len(caption_ids))\n",
    "}\n",
    "\n",
    "metric = eccv_caption.Metrics()\n",
    "scores = metric.compute_all_metrics(\n",
    "    i2t_retrieved_items=i2t,\n",
    "    t2i_retrieved_items=t2i,\n",
    "    target_metrics=[\"coco_5k_recalls\",\n",
    "                    \"eccv_map_at_r\", \"eccv_rprecision\", \"eccv_r1\"],\n",
    "    Ks=[1, 5, 10],\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"COCO 5K T2I recalls:\")\n",
    "print(f\"R@1: {scores['coco_5k_r1']['t2i']:.2f}\")\n",
    "print(f\"R@5: {scores['coco_5k_r5']['t2i']:.2f}\")\n",
    "print(f\"R@10: {scores['coco_5k_r10']['t2i']:.2f}\")\n",
    "print(\"ECCV T2I metrics:\")\n",
    "print(f\"Map@R: {scores['eccv_map_at_r']['t2i']:.2f}\")\n",
    "print(f\"R-P: {scores['eccv_rprecision']['t2i']:.2f}\")\n",
    "print(f\"R@1: {scores['eccv_r1']['t2i']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc524fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip2_rerank(blip2_model, blip2_vis, blip2_txt, captions, image_paths, t2i_rank, batch_size, device):\n",
    "    processed_images = []\n",
    "    total = len(image_paths)\n",
    "    batch_start = time.time()\n",
    "    for i in tqdm(range(total), desc=\"BLIP2 preprocess\"):\n",
    "        image = blip2_vis(Image.open(image_paths[i]).convert(\"RGB\"))\n",
    "        processed_images.append(image)\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"BLIP2 preprocess: {total} in {batch_time:.3f}s\")\n",
    "\n",
    "    reranked = []\n",
    "    total_caps = len(captions)\n",
    "    batch_start = time.time()\n",
    "    for i in tqdm(range(total_caps), desc=\"BLIP2 rerank\"):\n",
    "        cand = t2i_rank[i]\n",
    "        text_input = blip2_txt(captions[i])\n",
    "        scores = []\n",
    "        for j in range(0, len(cand), batch_size):\n",
    "            chunk = cand[j:j + batch_size]\n",
    "            images = torch.stack([processed_images[k] for k in chunk]).to(device)\n",
    "            text_batch = [text_input] * len(chunk)\n",
    "            with torch.no_grad():\n",
    "                logits = blip2_model(\n",
    "                    {\"image\": images, \"text_input\": text_batch}, match_head=\"itm\"\n",
    "                )\n",
    "            scores.append(logits[:, 1].float().cpu())\n",
    "        scores = torch.cat(scores, dim=0)\n",
    "        order = torch.argsort(scores, descending=True)\n",
    "        reranked.append([cand[idx] for idx in order.tolist()])\n",
    "\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"BLIP2 rerank: {total_caps} in {batch_time:.3f}s\")\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4874cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "BLIP2_IMAGE_BATCH = 2\n",
    "blip2_model, blip2_vis, blip2_txt = load_model_and_preprocess(\n",
    "    name=\"blip2_image_text_matching\",\n",
    "    model_type=\"pretrain\",\n",
    "    is_eval=True,\n",
    "    device=device,\n",
    ")\n",
    "blip2_model.eval()\n",
    "\n",
    "clip_t2i_rank = topk_faiss(text_feats, image_feats, RETRIEVE_K).tolist()\n",
    "t2i_rank_blip2 = blip2_rerank(\n",
    "    blip2_model,\n",
    "    blip2_vis[\"eval\"],\n",
    "    blip2_txt[\"eval\"],\n",
    "    captions,\n",
    "    images,\n",
    "    clip_t2i_rank,\n",
    "    BLIP2_IMAGE_BATCH,\n",
    "    device,\n",
    ")\n",
    "\n",
    "i2t_blip2 = {\n",
    "    image_ids[i]: [caption_ids[j] for j in i2t_rank[i]]\n",
    "    for i in range(len(image_ids))\n",
    "}\n",
    "t2i_blip2 = {\n",
    "    caption_ids[i]: [image_ids[j] for j in t2i_rank_blip2[i]]\n",
    "    for i in range(len(caption_ids))\n",
    "}\n",
    "\n",
    "metric = eccv_caption.Metrics()\n",
    "scores_blip2 = metric.compute_all_metrics(\n",
    "    i2t_retrieved_items=i2t_blip2,\n",
    "    t2i_retrieved_items=t2i_blip2,\n",
    "    target_metrics=[\"coco_5k_recalls\", \"eccv_map_at_r\", \"eccv_rprecision\", \"eccv_r1\"],\n",
    "    Ks=[1, 5, 10],\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"BLIP2 COCO 5K T2I recalls:\")\n",
    "print(f\"R@1: {scores_blip2['coco_5k_r1']['t2i']:.2f}\")\n",
    "print(f\"R@5: {scores_blip2['coco_5k_r5']['t2i']:.2f}\")\n",
    "print(f\"R@10: {scores_blip2['coco_5k_r10']['t2i']:.2f}\")\n",
    "print(\"BLIP2 ECCV T2I metrics:\")\n",
    "print(f\"Map@R: {scores_blip2['eccv_map_at_r']['t2i']:.2f}\")\n",
    "print(f\"R-P: {scores_blip2['eccv_rprecision']['t2i']:.2f}\")\n",
    "print(f\"R@1: {scores_blip2['eccv_r1']['t2i']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320adff0",
   "metadata": {},
   "source": [
    "### Qwen Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ed5cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce2330ea37f4001b0a99b4fea04536f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, \"/Users/weedcuper/Desktop/Degree/HSE_MS/Module3/ML-Project/reqs/Qwen3-VL-Embedding\")\n",
    "sys.path.insert(0, \"reqs/Qwen3-VL-Embedding\")\n",
    "from src.models.qwen3_vl_reranker import Qwen3VLReranker\n",
    "qwen_model = Qwen3VLReranker(model_name_or_path=\"Qwen/Qwen3-VL-Reranker-2B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9461e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_rerank(qwen_model, qwen_preprocess, captions, image_paths, t2i_rank, batch_size, device):\n",
    "    reranked = []\n",
    "    for i in tqdm(range(len(captions)), desc=\"Qwen rerank\"):\n",
    "        caption = captions[i]\n",
    "        cand_ids = t2i_rank[i]\n",
    "        scores = []\n",
    "        for s in range(0, len(cand_ids), batch_size):\n",
    "            batch_ids = cand_ids[s:s + batch_size]\n",
    "            docs = [{\"image\": image_paths[j]} for j in batch_ids]\n",
    "            inputs = {\"query\": {\"text\": caption}, \"documents\": docs}\n",
    "            scores.extend(qwen_model.process(inputs))\n",
    "        order = sorted(range(len(cand_ids)), key=lambda k: scores[k], reverse=True)\n",
    "        reranked.append([cand_ids[k] for k in order])\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25bc2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2300cb7642e439e84af26346b125bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen rerank:   0%|          | 0/25010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ironyoid/miniforge3/envs/qwen3vl/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2402: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 1016455.99it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 2009497.71it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 702704.73it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 1018776.78it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 550476.94it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 756150.08it/s]\n",
      "100%|██████████| 1261/1261 [00:00<00:00, 34539.84it/s]\n",
      "100%|██████████| 1332/1332 [00:00<00:00, 70909.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen COCO 5K T2I recalls:\n",
      "R@1: 0.60\n",
      "R@5: 0.81\n",
      "R@10: 0.86\n",
      "Qwen ECCV T2I metrics:\n",
      "Map@R: 0.41\n",
      "R-P: 0.49\n",
      "R@1: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip_t2i_rank = topk_faiss(text_feats, image_feats, RETRIEVE_K).tolist()\n",
    "t2i_rank_qwen = qwen_rerank(\n",
    "    qwen_model, None, captions, images, clip_t2i_rank, ALBEF_IMAGE_BATCH, device\n",
    ")\n",
    "\n",
    "i2t_qwen = {\n",
    "    image_ids[i]: [caption_ids[j] for j in i2t_rank[i]]\n",
    "    for i in range(len(image_ids))\n",
    "}\n",
    "t2i_qwen = {\n",
    "    caption_ids[i]: [image_ids[j] for j in t2i_rank_qwen[i]]\n",
    "    for i in range(len(caption_ids))\n",
    "}\n",
    "\n",
    "metric = eccv_caption.Metrics()\n",
    "scores_qwen = metric.compute_all_metrics(\n",
    "    i2t_retrieved_items=i2t_qwen,\n",
    "    t2i_retrieved_items=t2i_qwen,\n",
    "    target_metrics=[\"coco_5k_recalls\",\n",
    "                    \"eccv_map_at_r\", \"eccv_rprecision\", \"eccv_r1\"],\n",
    "    Ks=[1, 5, 10],\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"Qwen COCO 5K T2I recalls:\")\n",
    "print(f\"R@1: {scores_qwen['coco_5k_r1']['t2i']:.2f}\")\n",
    "print(f\"R@5: {scores_qwen['coco_5k_r5']['t2i']:.2f}\")\n",
    "print(f\"R@10: {scores_qwen['coco_5k_r10']['t2i']:.2f}\")\n",
    "print(\"Qwen ECCV T2I metrics:\")\n",
    "print(f\"Map@R: {scores_qwen['eccv_map_at_r']['t2i']:.2f}\")\n",
    "print(f\"R-P: {scores_qwen['eccv_rprecision']['t2i']:.2f}\")\n",
    "print(f\"R@1: {scores_qwen['eccv_r1']['t2i']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
