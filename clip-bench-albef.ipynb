{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4285043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import eccv_caption\n",
    "from omegaconf import OmegaConf\n",
    "from lavis.models import load_preprocess\n",
    "from lavis.models.albef_models.albef_retrieval import AlbefRetrieval\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ea3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/coco2014\"\n",
    "ANN_PATH = f\"{DATASET_PATH}/annotations/karpathy_test.json\"\n",
    "DATA_DIR = f\"{DATASET_PATH}/eccv/data\"\n",
    "MODEL_NAME = \"ViT-L-14\"\n",
    "PRETRAINED = \"openai\"\n",
    "IMAGE_BATCH = 64\n",
    "TEXT_BATCH = 256\n",
    "RETRIEVE_K = 50\n",
    "ALBEF_IMAGE_BATCH = 32\n",
    "ALBEF_CFG_PATH = \"configs/albef_retrieval_base.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d4b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_karpathy_test(ann_path, coco_root):\n",
    "    data = json.load(open(ann_path, \"r\", encoding=\"utf-8\"))\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    captions = []\n",
    "    caption_ids = []\n",
    "\n",
    "    for img in data[\"images\"]:\n",
    "        if img[\"split\"] != \"test\":\n",
    "            continue\n",
    "        img_path = os.path.join(coco_root, img[\"filepath\"], img[\"filename\"])\n",
    "        images.append(img_path)\n",
    "        image_ids.append(img[\"cocoid\"])\n",
    "        for sent in img[\"sentences\"]:\n",
    "            captions.append(sent[\"raw\"].strip())\n",
    "            caption_ids.append(sent[\"sentid\"])\n",
    "\n",
    "    return images, image_ids, captions, caption_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0726c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(model, preprocess, image_paths, batch_size, device):\n",
    "    feats = []\n",
    "    total = len(image_paths)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = [preprocess(Image.open(p).convert(\"RGB\"))\n",
    "                  for p in batch_paths]\n",
    "        image_input = torch.stack(images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_feats = model.encode_image(image_input)\n",
    "        feats.append(batch_feats)\n",
    "        # if (i // batch_size) % 10 == 0:\n",
    "        #     done = min(i + batch_size, total)\n",
    "        #     print(f\"Encoded images: {done}/{total}\")\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    return feats / feats.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349858e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(model, tokenizer, texts, batch_size, device):\n",
    "    feats = []\n",
    "    total = len(texts)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        tokens = tokenizer(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_feats = model.encode_text(tokens)\n",
    "        feats.append(batch_feats)\n",
    "        # if (i // batch_size) % 10 == 0:\n",
    "        #     done = min(i + batch_size, total)\n",
    "        #     print(f\"Encoded captions: {done}/{total}\")\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    return feats / feats.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f76b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_faiss(text_feats, img_feats, k):\n",
    "    txf = text_feats.detach().cpu().numpy().astype(\"float32\", copy=False)\n",
    "    imf = img_feats.detach().cpu().numpy().astype(\"float32\", copy=False)\n",
    "    txf = np.ascontiguousarray(txf)\n",
    "    imf = np.ascontiguousarray(imf)\n",
    "    k = min(k, imf.shape[0])\n",
    "    index = faiss.IndexFlatIP(imf.shape[1])\n",
    "    index.add(imf)\n",
    "    _, indices = index.search(txf, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366a2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def albef_rerank(albef_model, albef_preprocess, captions, image_paths, t2i_rank, batch_size, device):\n",
    "    image_feats = []\n",
    "    total = len(image_paths)\n",
    "    batch_start = time.time()\n",
    "    for i in tqdm(range(0, total, batch_size), desc=\"ALBEF image feats\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = [albef_preprocess(Image.open(p).convert(\"RGB\"))\n",
    "                  for p in batch_paths]\n",
    "        image_input = torch.stack(images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_feats = albef_model.visual_encoder.forward_features(\n",
    "                image_input)\n",
    "        image_feats.append(batch_feats.cpu())\n",
    "    image_feats = torch.cat(image_feats, dim=0)\n",
    "    batch_time = time.time() - batch_start\n",
    "\n",
    "    print(f\"ALBEF image feats: {total} in {batch_time:.3f}s\")\n",
    "    \n",
    "    reranked = []\n",
    "    total_caps = len(captions)\n",
    "    batch_start = time.time()\n",
    "    for i in tqdm(range(total_caps), desc=\"ALBEF rerank\"):\n",
    "        cand = t2i_rank[i]\n",
    "        img_feat = image_feats[cand].to(device)\n",
    "        encoder_att = torch.ones(\n",
    "            img_feat.size()[:-1], dtype=torch.long).to(device)\n",
    "        text_input = albef_model.tokenizer(\n",
    "            [captions[i]] * len(cand),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=35,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = albef_model.text_encoder(\n",
    "                text_input.input_ids,\n",
    "                attention_mask=text_input.attention_mask,\n",
    "                encoder_hidden_states=img_feat,\n",
    "                encoder_attention_mask=encoder_att,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            scores = albef_model.itm_head(\n",
    "                output.last_hidden_state[:, 0, :])[:, 1]\n",
    "        order = torch.argsort(scores, descending=True)\n",
    "        reranked.append([cand[j] for j in order.tolist()])\n",
    "\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"ALBEF rerank: {total_caps} in {batch_time:.3f}s\")\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ed5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Test images: 5000\n",
      "Test captions: 25010\n",
      "Test captions: 25010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded in 299.5s\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "images, image_ids, captions, caption_ids = load_karpathy_test(\n",
    "    ANN_PATH, DATASET_PATH)\n",
    "print(f\"Test images: {len(images)}\")\n",
    "print(f\"Test captions: {len(captions)}\")\n",
    "print(f\"Test captions: {len(caption_ids)}\")\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    MODEL_NAME, pretrained=PRETRAINED, device=device)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "t0 = time.time()\n",
    "image_feats = encode_images(model, preprocess, images, IMAGE_BATCH, device)\n",
    "text_feats = encode_texts(model, tokenizer, captions, TEXT_BATCH, device)\n",
    "print(f\"Encoded in {time.time() - t0:.1f}s\")\n",
    "\n",
    "i2t_rank = topk_faiss(image_feats, text_feats, RETRIEVE_K).tolist()\n",
    "t2i_rank = topk_faiss(text_feats, image_feats, RETRIEVE_K).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32b06b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25010, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(t2i_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf6c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weedcuper/miniconda3/envs/openclip/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 256 to 576\n",
      "reshape position embedding from 256 to 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALBEF image feats: 100%|██████████| 157/157 [02:56<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBEF image feats: 5000 in 191.548s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALBEF rerank: 100%|██████████| 25010/25010 [1:42:56<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBEF rerank: 25010 in 6176.617s\n"
     ]
    }
   ],
   "source": [
    "albef_cfg = OmegaConf.load(ALBEF_CFG_PATH)\n",
    "albef_model = AlbefRetrieval.from_config(albef_cfg.model)\n",
    "albef_model.eval()\n",
    "albef_model = albef_model.to(device)\n",
    "albef_vis, _ = load_preprocess(albef_cfg.preprocess)\n",
    "\n",
    "t2i_rank = albef_rerank(\n",
    "    albef_model, albef_vis[\"eval\"], captions, images, t2i_rank, ALBEF_IMAGE_BATCH, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f195d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2t = {\n",
    "    image_ids[i]: [caption_ids[j] for j in i2t_rank[i]]\n",
    "    for i in range(len(image_ids))\n",
    "}\n",
    "t2i = {\n",
    "    caption_ids[i]: [image_ids[j] for j in t2i_rank[i]]\n",
    "    for i in range(len(caption_ids))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5134a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 1744573.66it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 2474515.63it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 1091243.63it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 1653748.86it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 1073590.66it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 1353699.97it/s]\n",
      "100%|██████████| 1261/1261 [00:00<00:00, 60587.17it/s]\n",
      "100%|██████████| 1332/1332 [00:00<00:00, 121154.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO 5K T2I recalls:\n",
      "R@1: 0.52\n",
      "R@5: 0.76\n",
      "R@10: 0.84\n",
      "ECCV T2I metrics:\n",
      "Map@R: 0.41\n",
      "R-P: 0.50\n",
      "R@1: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metric = eccv_caption.Metrics()\n",
    "scores = metric.compute_all_metrics(\n",
    "    i2t_retrieved_items=i2t,\n",
    "    t2i_retrieved_items=t2i,\n",
    "    target_metrics=[\"coco_5k_recalls\",\n",
    "                    \"eccv_map_at_r\", \"eccv_rprecision\", \"eccv_r1\"],\n",
    "    Ks=[1, 5, 10],\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"COCO 5K T2I recalls:\")\n",
    "print(f\"R@1: {scores['coco_5k_r1']['t2i']:.2f}\")\n",
    "print(f\"R@5: {scores['coco_5k_r5']['t2i']:.2f}\")\n",
    "print(f\"R@10: {scores['coco_5k_r10']['t2i']:.2f}\")\n",
    "print(\"ECCV T2I metrics:\")\n",
    "print(f\"Map@R: {scores['eccv_map_at_r']['t2i']:.2f}\")\n",
    "print(f\"R-P: {scores['eccv_rprecision']['t2i']:.2f}\")\n",
    "print(f\"R@1: {scores['eccv_r1']['t2i']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed5cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa24716d9a944eae86082e96f8916432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b43087f3246413da2d1ac7a56bfc522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/weedcuper/Desktop/Degree/HSE_MS/Module3/ML-Project/reqs/Qwen3-VL-Embedding\")\n",
    "from src.models.qwen3_vl_reranker import Qwen3VLReranker\n",
    "model = Qwen3VLReranker(model_name_or_path=\"Qwen/Qwen3-VL-Reranker-8B\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
